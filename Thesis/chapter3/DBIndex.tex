\newcommand{\DBIndex}{DBIndex}
\newcommand{\blockset}{{\cal B}}
\newcommand{\clusterset}{{\cal C}}
\section{Dense Block Index}\label{gw:sec:db-index}
A straightforward approach to process a graph window query 
$Q = (G, W, \Sigma)$, %where $G = (V,E)$,
is to dynamically compute the window $W(v)$ for each vertex $v \in V$ and
its aggregation 
$\Sigma_{v' \in W(v)} v'$ 
independently from other vertexes. We refer to this approach as \emph{Non-Indexed} method.

Given that many of the windows would share a large number of common nodes (e.g., the $k$-hop windows of two adjacent 
vertexes),
such a simple approach would be very inefficient due to the lack of sharing of the computation. 

To efficiently evaluate graph window queries, we propose an index technique named \emph{Dense Block Index} (\textit{\DBIndex}), which achieves both space and query efficiency. 
The main idea of \DBIndex\ is to try to reduce the aggregation cost by identifying subsets of nodes that are shared by more than one window 
so that the aggregation for the shared nodes could be computed only once instead of multiple times.

For example, consider a graph window query on the social graph in Figure~\ref{fig:attributed} 
using the 1-hop window function.
We have $W(B) = \{A,B,D,F\}$ and $W(C) = \{A,C,D,E,F\}$ sharing three common nodes $A$, $D$, and $F$.
By identifying the set of common nodes $S=\{A,D,F\}$, its aggregation 
$\Sigma_{v \in S} v$ can be computed only once
and then reused to compute the aggregation for $\Sigma_{v \in W(B)} v$ and $\Sigma_{v \in W(C)} v$.

%Given a graph window query $Q = (G, W, \Sigma, A)$ on a graph $G=(V,E)$,
Given a window function $W$ and a graph $G=(V,E)$,
we refer to a non-empty subset $B \subseteq V$ as a {\it block}.
Moreover, if $B$ contains at least two nodes and $B$ is contained by at least two different windows
(i.e., there exists $v_1 \neq v_2 \in V$, s.t. $B \subseteq W(v_1)$, and $B \subseteq W(v_2)$),
then $B$ is referred to as a {\it dense block}.
Thus, in the last example, $\{A,D,F\}$ is a dense block.

We say that a window $W(X)$ is {\it covered} by a collection of disjoint blocks $\{B_1,\cdots,B_n\}$
if the set of nodes in the window $W(X)$ is equal to the union of all nodes in the collection of disjoint blocks;
i.e., $W(X) = \bigcup_{i=1}^{n} B_i$ and $B_i \cap B_j = \emptyset$ if $i \neq j$.

To maximize the sharing of aggregation for a graph window query, 
the objective of \DBIndex\ is to identify a small set of blocks $\blockset$ such that
for each $v \in V$, $W(v)$ is covered by a small subset of disjoint blocks in $\blockset$.
Clearly, the cardinality of $\blockset$ is minimized if $\blockset$ contains a few large dense blocks.

Thus, given a window function $W$ and a graph $G=(V,E)$,
a \DBIndex\ to evaluate $W$ on $G$ consists of three components in the form of a bipartite graph.
The first component is a collection of nodes (i.e., $V$);
the second component is a collection of blocks; i.e., $\blockset = \{B_1,\cdots,B_n\}$ where each $B_i \subseteq V$;
and the third component is a collection of links from blocks to nodes
such that if a set of blocks $B(v) \subseteq \blockset$ is linked to a node $v \in V$,
then $W(v)$ is covered by $B(v)$.
Note that a \DBIndex\ is independent of the aggregate functions (i.e., $\Sigma$).

Figure~\ref{fig:dbi_agg}(a) shows an example of a \DBIndex\ with respect to the social graph in Figure~\ref{fig:attributed} and the 1-hop window function.
Note that the index consists of a total of seven blocks of which three of them are dense blocks.

\begin{figure}[t]
\centerline{
	\includegraphics[width=0.8\textwidth]{chapter3/dbi_process} 
	}
	\caption{Window query processing using DBIndex. (a) provides the DBIndex for 1-hop window query in Figure~\ref{fig:attributed}; (b) shows the partial aggregate results based on the dense block; (c) provides the final aggregate value of each window.}
	\label{fig:dbi_agg}
\end{figure}

\subsection{Query Processing with \DBIndex}
Given a \DBIndex\ with respect to a graph $G$ and a window function $W$, a graph window query $Q = (G, W, \Sigma)$ is processed by the following two steps.
First, for each block $B_i$ in the index, we compute the aggregation (denoted by $T_i$) over all the nodes in $B_i$,
i.e., $T_i = \Sigma_{v \in B_i} v$. 
Thus, each $T_i$ is a partial aggregate value.
Next, for each window $W(v)$, $v \in V$, the aggregation for the window is computed by aggregating over all the partial aggregates associated with the blocks linked to $W(v)$.
In other words, if $B(v)$ is the collection of blocks linked to $W(v)$, 
then the aggregation for $W(v)$ is computed by $\Sigma_{B_i \in B(v)} T_i$. 

Consider again the \DBIndex\ shown in Figure~\ref{fig:dbi_agg}(a) 
defined with respect to the social graph in Figure~\ref{fig:attributed} and the 1-hop window function.
Figure~\ref{fig:dbi_agg}(b) shows how the index is used to evaluate the graph window query $(G, W, \mathtt{sum}(Posts))$
where each block is labeled with its partial aggregate value, and Figure~\ref{fig:dbi_agg}(c) shows the final query results.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{\DBIndex\  Construction} 

In this section, we discuss the construction of the \DBIndex\ (with respect to a graph $G=(V,E)$ and a window function $W$) which faces two key challenges.

The first challenge is the time complexity of the index construction. 
From the discussion of query processing using \DBIndex , 
we note that the number of aggregations is determined by both the number of blocks as well as the number of links in the index; 
the former determines the number of partial aggregates to compute
while the latter determines the number of aggregations of the partial aggregate values.
Thus, to maximize the shared aggregations using \DBIndex\ , both the number of blocks in the index as well as the number of blocks covering each window should be minimized. 
%As discussed, to maximize the shared aggregation computations using \DBIndex,
%both the number of blocks in the index as well as the number of blocks covering each window should be minimized.
However, finding the optimal \DBIndex\ to minimize this objective is NP-hard\footnote{
Note that a simpler variation of our problem has been proven to be NP-hard \cite{vassilevska2004finding}.}.
Therefore, effective heuristics are needed to construct the \DBIndex.

The second challenge is the space complexity of the index construction.
In order to identify large dense blocks to optimize the query processing,
a straightforward approach is to first derive the window $W(v)$ for each vertex $v \in V$ and
then use this derived information to identify large dense blocks.
However, this direct approach incurs a high space complexity of $O(|V|^2)$.
Therefore, a more space-efficient approach is needed in order to scale to large graphs.

In this section, we present two heuristic approaches, namely {\it Min-hash Clustering (MC)} and {\it Estimated Min-hash Clustering (EMC)}, to construct \DBIndex . The first approach $MC$ is to construct
a \DBIndex\ for general window functions; While the second approach $EMC$ is 
to construct a \DBIndex\ specifically for $k$-hop window function. Compared to $MC$, $EMC$
adopts a heuristic to speed up the index construction at the expense of sacrificing 
the ``quality'' of the dense blocks (in terms of their sizes).

%The second approach is designed to improve the efficiency of the first approach MC for constructing \DBIndex\
%(with respect to $k$-hop window function) by using some heuristics at the expense of possibly sacrificing 
%the ``quality'' of the dense blocks (in terms of their sizes).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Min-hash clustering (MC)}
To reduce both the time and space complexities for the index construction,
instead of trying to identify large dense blocks among a large collection of windows,
we first partition all the windows 
into a number of smaller clusters of 
similar window contents and then identify large dense blocks 
from each of the smaller clusters.
Intuitively, two windows are considered to 
be similar if they share a large subset of nodes.
We apply the well-known {\it Min-hash based Clustering (MC)} algorithm~\cite{broder1997syntactic} 
to partition the windows into clusters of similar windows.
The Min-hash clustering algorithm is based on the
\emph{Jaccard Coefficient} which measures the similarity of two sets.  
Given the two window $W(v)$ and $W(u)$, $u,v \in V$,
their \emph{Jaccard Coefficient} is given by
\begin{equation} \label{eq:jacc_sim}
	J(u,v) = \frac{|W(u) \cap W(v)|}{|W(u) \cup W(v)|}
\end{equation}
The \emph{Jaccard Coefficient} ranges from 0 to 1, where a larger value indicates
a higher similarity.

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{chapter3/dbi_indexing.pdf}
\caption{DBIndex construction over social graph in Figure~\ref{fig:attributed}. (a) shows two clusters after MinHash clustering. (b) shows inverted window list for each node. (c) shows the dense blocks via equivalent node merging. (d) provides the final DBIndex.}
\label{fig:dbi-indexing}
\end{figure}


Our heuristic approach to construct \DBIndex\ $I$ operates as follows.
Let $nodes(I)$, $blocks(I)$, and $links(I)$ denote, respectively,
the collection of nodes, blocks, and links that form $I$.
Initially, we have $nodes(I) = V$,
$blocks(I) = \emptyset$,
and
$links(I) = \emptyset$.
%
The first step is to partition the vertexes in $V$ into clusters
using Min-hash algorithm such that vertexes with similar windows belong to the same cluster. 
For each vertex $v \in V$, we first derive its window $W(v)$ by an appropriate traversal of the graph $G$.
Next, we compute a hash signature (denoted by $H(v)$) for $W(v)$ based on applying $m$ 
hash functions on the set $W(v)$.
Vertexes with identical hash signatures are considered 
to have highly similar windows and are grouped into the same cluster.
To ensure that our approach is scalable,
we do not retain $W(v)$ in memory after its hash signature $H(v)$ has been computed,
i.e., our approach does not materialize all the windows in the memory to avoid high space complexity.
Let $\clusterset = \{C_1,C_2,\cdots\}$ denote the collection of clusters obtained from the first step,
where each $C_i$ is a subset of vertexes.
%A cluster is classified as {\it trivial} if it contains only one window; otherwise, it is classified as {\it non-trivial}.

The second step is to identify dense blocks from each of the clusters computed in the first step.
%For each cluster $C_i \in \clusterset$,
%let $N_i$ denote the union of all the nodes in all the windows in $C_i$;
%i.e., $N_i = \{v \in W(u) |\ W(u) \in C_i\}$.
The identification of dense blocks in each cluster $C_i$ is based on the notion of node equivalence defined as follows.
Two distinct nodes $u, v$ are defined to be equivalent (denoted by $u \equiv v$)
if $u$ and $v$ are both contained in the same set of windows,
i.e., for every window $W(x), x \in C_i$, $u \in W(x)$ if and only if $v \in W(x)$.
Based on this notion of node equivalence, $C_i$ is partitioned into blocks of equivalent nodes.
To perform this partitioning, we need to again traverse the graph for each vertex $v \in C_i$ to 
determine its window $W(v)$\footnote{
Note that although we could have avoided deriving $W(v)$ a second time if we had materialized all the derived windows the first time, our approach is designed to avoid such a high space complexity at the cost of computing each $W(v)$ twice. We present an optimization in Section~\ref{sec:optimized} to avoid the recomputation cost.
}.
%However, since $C_i$ is now  a smaller cluster of nodes, we can now materialize all the windows for the nodes in $C_i$ in memory without exceeding the memory space.

However, since $C_i$ is now a smaller cluster of nodes, we can now materialize all the windows for the vertexes in $C_i$ in memory. In the event that a cluster $C_i$ is still too large for all its vertex windows to be materialized in main memory, we can re-partition $C_i$ into equal size sub-clusters. This re-partition process can be recursively performed until the sub clusters created are small enough such that the windows for all nodes in the sub clusters fit in memory. 

Recall that a block $B$ is a dense block if $B$ contains at least two nodes and
$B$ is contained in at least two windows.
Thus, we can classify the nodes in each $C_i$ as either dense or non-dense nodes:
a node $v \in C_i$ is classified as a {\it dense node} if $v$ is contained in a dense block;
otherwise, $v$ is a non-dense node.

For each dense block $B$ in $C_i$,
we update the blocks and links in the \DBIndex\ $I$ as follows:
we insert $B$ into $blocks(I)$ if $B \not\in blocks(I)$,
and we insert $(B,v)$ into $links(I)$
for each $v \in C_i$ where $B \subseteq W(v)$.
If all the blocks in $C_i$ are dense blocks, 
then we are done with identifying dense blocks in $C_i$;
otherwise, there are two cases to consider.
%otherwise, we update $C_i$ by removing all the dense nodes from $C_i$ and recursively apply the
%two-step procedure on the updated $C_i$ to try to identify more dense blocks.
For the first case, if all the nodes in $C_i$ are non-dense nodes,
then we also terminate the process  of identifying dense blocks in $C_i$
and update the blocks and links in the \DBIndex\ $I$ as before:
we insert each non-dense block $B$ into $blocks(I)$,
and we insert $(B,v)$ into $links(I)$ for each $v \in C_i$ where $B \subseteq W(v)$.
For the second case, if $C_i$ has a mixture of dense and non-dense nodes,
we remove the dense nodes from $C_i$ and recursively identify dense blocks in the remaining part of $C_i$
following the above two-step procedure.

Note that since the blocks are identified independently from each cluster,
it might be possible for the same block to be identified from different clusters.
We avoid duplicating the same block in $blocks(I)$ by checking that a block $B$ is not already in $blocks(I)$
before inserting it into $blocks(I)$.
The details of the construction algorithm are shown as Algorithms
\ref{algo:k-hop-dbi},
\ref{algo:identify},
and
\ref{algo:refine}.
\begin{algorithm}
\caption{CreateDBIndex}
\begin{algorithmic}[1]  \small
%\FUNCTION $dbiGen$
\Require Graph $G=(V,E)$, window function $W$
%\Ensure \DBIndex\ $I$
\State Initialize \DBIndex\ $I$: $nodes(I)=V$, $blocks(I)=\emptyset$, $links(I)=\emptyset$
\ForAll{$v \in V$}
	\State Traverse $G$ to determine $W(v)$
	\State Compute the hash signature $H(v)$ for $W(v)$
\EndFor
\State Partition $V$ into clusters $\clusterset = \{C_1,C_2,\cdots\}$ based on hash signatures $H(v)$
\ForAll{$C_i \in \clusterset$}
	\ForAll{$v \in C_i$} 
		\State Traverse $G$ to determine $W(v)$
	\EndFor
	\State {\tt IdentifyDenseBlocks} $(I,G,W,C_i)$
\EndFor \\
\Return $I$
\end{algorithmic}
\label{algo:k-hop-dbi}
\end{algorithm}

%begin{algorithm}
%\caption{IdentifyDenseBlocks}
%\begin{algorithmic}[1]
%\REQUIRE \DBIndex\ $I$, Graph $G=(V,E)$, window function $W$, a cluster $C_i \subseteq V$
%%\ENSURE \DBIndex
%\STATE Partition $C_i$ into blocks based on node equivalence
%\STATE Initialize $DenseNodes = \emptyset$
%\FORALL{dense block $B$}
% \STATE Insert $B$ into $blocks(I)$ if $B \not\in blocks(I)$
% \STATE Insert $(B,v)$ into $links(I)$ for each $v \in C_i$ where $B \subseteq W(v)$
% \STATE $DenseNodes = DenseNodes \cup B$
%\ENDFOR
%\IF{($DenseNodes = \emptyset$)}
% \FORALL{block $B$}
%  \STATE Insert $B$ into $blocks(I)$ if $B \not\in blocks(I)$
%  \STATE Insert $(B,v)$ into $links(I)$ for each $v \in C_i$ where $B \subseteq W(v)$
% \ENDFOR
%\ELSIF{($C_i - DenseNodes \neq \emptyset$)}
% \IF{($C_i \neq DenseNodes$)}
%  \STATE {\tt RefineCluster} $(I,G,W,C_i - DenseNodes)$
%  \ENDIF
%\ENDIF
%\end{algorithmic}
%\label{algo:identify}
%\end{algorithm}

\begin{algorithm}
\caption{IdentifyDenseBlocks}
\begin{algorithmic}[1] \small
\Require \DBIndex\ $I$, Graph $G=(V,E)$, window function $W$, a cluster $C_i \subseteq V$
%\ENSURE \DBIndex
\State Partition $C_i$ into blocks based on node equivalence
\State Initialize $DenseNodes = \emptyset$
\ForAll{dense block $B$}
	\State Insert $B$ into $blocks(I)$ if $B \not\in blocks(I)$
	\State Insert $(B,v)$ into $links(I)$ for each $v \in C_i$ where $B \subseteq W(v)$
	\State $DenseNodes = DenseNodes \cup B$
\EndFor
\If{($DenseNodes = \emptyset$)}
	\ForAll{block $B$}
		\State Insert $B$ into $blocks(I)$ if $B \not\in blocks(I)$
		\State Insert $(B,v)$ into $links(I)$ for each $v \in C_i$ where $B \subseteq W(v)$
	\EndFor
\ElsIf{($C_i - DenseNodes \neq \emptyset$)}
	\If{($C_i \neq DenseNodes$)}
		\State {\tt RefineCluster} $(I,G,W,C_i - DenseNodes)$
	\EndIf
\EndIf
\end{algorithmic}
\label{algo:identify}
\end{algorithm}

Figure~\ref{fig:dbi-indexing} 
illustrates the construction of the DBIndex with respect to the social graph in 
Figure~\ref{fig:attributed}(a) and 1-hop window using the MC algorithm.
First, the set of graph vertexes are partitioned into clusters using Min-hash clustering;
Figure~\ref{fig:dbi-indexing}(a)
shows that the set of vertexes $V = \{A, B, C, D, E, F \}$ are partitioned into two clusters $C_1=\{A, B, C\}$ and $C_2=\{D, E, F\}$. 

For example, cluster $C_1$ in 
Figure~\ref{fig:dbi-indexing}(b) shows the inverted list representing
the node $n$ and the vertexes $v \in C_1$ whose windows contains $n$, i.e., $\{v | n \in W(v) \}$.
%
%shows for each $v \in C_1$, the set of vertexes whose windows contain $v$;
%i.e., $\{u |\ v \in W(u)\}$.
%Similarly, Cluster $C_2$ in Figure~\ref{fig:dbi-indexing} (b)
%shows for each $v \in C_2$, the set of vertexes whose windows contain $v$.
Consider the identification of dense blocks in cluster $C_1$.
As shown in Figure~\ref{fig:dbi-indexing} (c), based on the notion of equivalence nodes,
cluster $C_1$ is partitioned into three blocks of equivalent nodes:
$B_1=\{A,D,F\}$, $B_2=\{B\}$, and $B_3\{C,E\}$.
Among these three blocks, only
$B_1$ and $B_3$ are dense blocks.
The MC algorithm then tries to repartition the window $A,B,C$ using non-dense nodes in $C_1$,
(i.e., $B_2$). Since $B_2$ is the only non-dense node, it directly outputs.
%the remaining non-dense nodes in $C_1$ (i.e., $\{B\}$).
At the end of processing cluster $C_1$,
the DBIndex $I$ is updated as follows:
$blocks(I) = \{B_1, B_2, B_3\}$ 
and
$links(I) = \{ (B_1,\{A,B,C\}), (B_2, \{A,B\}),$, $(B_3, \{A,C\}) \}$. 
The identification of dense blocks in cluster $C_2$ 
is of similar process.

We find it is non-trivial to precisely analyze the complexity of Algorithm~\ref{algo:k-hop-dbi}.
Here, we only offer a brief analysis. Suppose the Min-hash cost is $H$ and 
the total cost for collecting window function for all vertexes is $B$, 
Lines 1-5 has the complexity of $O(H + B)$.  
Lines 7-10 has the complexity of $O(B)$. A single execution of Algorithm~\ref{algo:identify}  has the  complexity of $O(|V|)$, since we can simply partition nodes using hashing. Suppose the iteration runs for $K$ times, the total cost for Algorithm~\ref{algo:identify} and Algorithm~\ref{algo:refine} is $O(K|V|)$. Therefore the overall complexity of Algorithm~\ref{algo:k-hop-dbi} is $O(H+2*B + O(K|V|))$. $H$ depends on the number of vertex-window mappings for a given query and $B$ depends on the window functions.
As we demonstrate in Section~\ref{gw:sec:exp}, for $k$-hop window, the $H$ and $B$ are the major factors in the index construction. To reduce the construction time, we design further optimization techniques for the $k$-hop window.


\begin{algorithm}
\caption{RefineCluster}
\begin{algorithmic}[1]
\Require \DBIndex\ $I$, Graph $G=(V,E)$, window function $W$, a cluster $C \subseteq V$
%\ENSURE \DBIndex
\ForAll{$v \in C$}
	\State Compute the hash signature $H(v)$ for $W(v) \cap C$
\EndFor
\State Partition $C$ into clusters $\clusterset = \{C_1,C_2,\cdots\}$ based on hash signatures $H(v)$
\ForAll{$C_i \in \clusterset$}
	\State {\tt IdentifyDenseBlocks} $(I,G,W,C_i)$
\EndFor
\end{algorithmic}
\label{algo:refine}
\end{algorithm}

\subsubsection{Estimated min-hash clustering (EMC)}
\label{sec:optimized}

The MC approach described in the previous section requires the window of each node (i.e., $W(v), v \in V$)
to be computed twice in order to avoid the high space complexity of materializing all the windows in main memory.
For $k$-hop window function with a large value of $k$, the cost of graph traversal 
to compute all $k$-hop windows
could incur a high computation overhead. Moreover, the cost of initial Min-hash in MC approach equals to the initial number of vertex-window mappings, which is of the same order as graph traversal. For the larger hops, Min-hash clustering would incur high computation cost.

To address these issues, we present a more efficient approach,
referred to as {\it Estimated Min-hash Clustering (EMC)}, 
to optimize the construction of the \DBIndex\ for $k$-hop window function with larger $k$.

The key idea behind $EMC$ is based on the observation that for any two nodes $u, v \in V$,
if their $m$-hop windows, $W_m(u)$ and $W_m(v)$, are highly similar 
and they are grouped into the same cluster, 
then it is likely that the $n$-hop windows of these two nodes, where $n > m$,
would also be highly similar and grouped into the same cluster.

Using the above observation, we could reduce the overhead for constructing a \DBIndex\ with
respect to a $k$-hop window 
function by clustering the vertexes based on their $k'$-hop windows, where $k' < k$, instead of their $k$-hop windows.

To reduce the overhead of window computations,
our $EMC$ approach is similar to the MC approach except   
for the first round of window computations
(Line 3 in Algorithm \ref{algo:k-hop-dbi}):
$EMC$ uses lower hop windows to approximate $k$-hop windows 
to cluster the vertexes in $V$.
Thus, the hash signatures used for partitioning 
$V$ are based on lower hop windows.
This approximation clearly has the advantage of improved 
time-efficiency as traversing and hashing on lower hop window is of
order of magnitude faster. 
For the extreme case, adopting 1-hop window of a node $v$ requires only accessing the adjacent nodes of $v$. 
The tradeoff for the improved efficiency is the reduced ``quality''
of the dense blocks (in terms of their sizes).
However, our experimental results show that this reduction in 
quality is only marginal which makes this approximation worthy.

\subsubsection{Justification of heuristic}
In the following, we justify of our heuristic by make
an assumption on the independence\footnote{Although the assumption may not always hold in reality, it makes the analysis feasible. We also conduct an empirical evaluation on real datasets in Section~\ref{sec:mc_vs_emc} to indicate the effectiveness of our heuristic.} of the 
vertexes in a graph. First, we provide the following theorem which
links a set of connected vertexes to the newly discovered vertex by one-hop
expansion.

\begin{theorem}
Let $S$ be a collection of connected vertexes. Let $T$ be the collection
of newly discovered vertexes from one-hop expansion of $S$. Then the ratio of 
$|T|/|S|$ decreases with respect to $|S|$.
\end{theorem}
\begin{proof}

Consider a random variable $Y_i$ which indicates the newly 
discovered vertexes from one-hop expansion from vertex $i$. 
Then the probability of $|Y_i|=y$ is can be analyzed as follows: there 
are $d_i$ edges for vertex $i$. Since $|Y_i|$ is connected with $S$, one edge
is fixed to link with a vertex in $S$. There are remaining $d_i-1$ edges with
$y$ edges linked to the new vertexes. In total, there are $|V|-1 \choose d_i -1$
combinations with $d_i$ edges. Therefore, the probability can be written as:
\begin{equation}
P(Y_i = y| v_i \in S) = \frac{{|S| -1 \choose d_i - y -1}{|V|-|S| \choose y}}{{|V|-1 \choose d_i -1}}
\end{equation}
Thus, the expectation of $Y_i$ is:
\begin{equation}
\begin{split}
E(Y_i|v_i \in S) & = \Sigma( y P(Y_i = y| v_i \in S) )\\
	& = \Sigma_{y=1}^{y=d_i -1} ( \frac{{|S| -1 \choose d_i - y -1}{|V|-|S| \choose y}}{{|V|-1 \choose d_i -1}} y )\\
	& = \Sigma_{y=1}^{y=d_i -1} (\frac{{|S| -1 \choose d_i - y -1}{|V|-|S| -1 \choose y - 1}}{{|V|-1 \choose d_i -1}}  (|V|-|S|))\\
	& = (|V|-|S|)  \Sigma_{y=1}^{y=d_i -1}\frac{{|S| -1 \choose d_i - y -1}{|V|-|S| -1 \choose y - 1}}{{|V|-1 \choose d_i -1}} \\
	& = (|V|-|S|)  \frac{{|V|-2 \choose d_i - 2}}{{|V|-1 \choose d_i -1}} = \frac{(|V|-|S|)(d_i-1)}{|V| - 1}
\end{split}
\end{equation}
Taking the expectation over all vertexes in $S$, we can find the expectation of $E(Y_i|S) = \frac{(|V|-|S|)(\overline{d}-1)}{|V| - 1}$,
where $\overline{d}$ is the average degree of the graph. Let %$T=\cup_{i=1}^{i=|S|} Y_i$ 
$T$ be the number of newly discovered vertexes for one-hop expansion of the entire set $S$.  
Since each $Y_i$ is independent,
it follows that $E(T) = E(\Sigma_{i=1}^{i=|S|}Y_i|S) = \Sigma_{i=1}^{i=|S|}E(Y_i|S) = \frac{|S|(|V|-|S|)(\overline{d} -1)}{|V|-1}$.  Let $\alpha$ be the ration of $\frac{E(T)}{|S|}$. It follows 
$\alpha= \frac{\overline{d}-1}{|V|-1}(|V|-|S|)$. As $\overline{d}$, $V$ are constants, $\alpha$ decreases linearly with respect to $|S|$.
%Taking derivative on $|S|$, it follows:$\frac{dE(T|S)}{d|S|} = \frac{\overline{d}-1}{|V|-1}(|V|-|S|) = \lambda_1-\lambda_2|S|$, where $\lambda_1, \lambda_2 >0$ are constants with respect to $|S|$. Therefore,
%the expected number of newly discovered vertexes $|T|$ is inversely proportional (i.e., $-\lambda_2$)
%to $|S|$.
\end{proof}

Next, we consider the Jaccard coefficient of two vertexes $u,v$'s windows. Let $J_k(u,v)$ be the 
Jaccard coefficient of $u,v$ at hop $k$. We use $I_k$ to denote $W_k(u) \cap W_k(v)$, and $U_k$ to denote
$W_k(u) \cup W_k(v)$. Therefore, $J_k(u,v)$ can be represented as $\frac{|I_k|}{|U_k|}$. Let
$J_{k+1}(u,v) =\frac{|I_k| + \alpha_I|I_k|}{|U_k|+ \alpha_U|I_U|}$, where $\alpha_I$ and $\alpha_U$ 
be the ratio of newly discovered vertexes versus original set of vertexes.
Since $|U_k| > |I_k|$, by the above theorem, $1+\alpha_U < 1+\alpha_I$. Therefore, $J_{k+1}(u,v)=\frac{|I_k| + \alpha_I|I_k|}{|U_k|+ \alpha_U|I_U|}= \frac{1+\alpha_I}{1+\alpha_U}\cdot\frac{|I_k|}{|I_U|} > J_k(u,v)$. This indicates that
the expected Jaccard coefficient of the two vertexes increases with respect to $k$, which justifies
our heuristic. 
It is notable that, although our analysis of the heuristic relies on the independence of vertexes,
our experiments show that the heuristic is effective in many real datasets.

%\subsubsection{Justification of Heuristic}
%In the following, we show the theoretical justification of our heuristic: 
%the Jaccard coefficient between two vertex's windows 
%is likely to increase with respect to the number of hops 
%for a large class of graphs. We assume that the degree of vertexes
%follows the same distribution, which is true in most real-networks and random graph models\footnote{E.g. Social network
%and Preferential Attachment model follow
%power-law distribution. However, our analysis works for other distribution as well.}.  
%This implies we can analyze vertexes with their neighborhoods structure using a unified way. 
%
%
%Our heuristic is based on the observation that...
%
%Our intuition is that when a vertex's window expands, the the proportion of the newly discovered
%vertex decreases. This is formulated as the following theorem:
%
%\begin{theorem}
%Let $S$ be a collection of connected vertexes. Let $T$ be the collection
%of newly discovered vertexes from one-hop expansion of $S$. Then the expectation
%of $|T|$ is inversely proportional to $|S|$.
%\end{theorem}
%
%
%
%%We use $d_i$ to indicate the degree of vertex $i$. For any vertex pair $(u,v)$, their 
%%intersection on $k$-hop window consists of three part. We name them using $A=W_k(u)-W_k(v)$, 
%%$B=W_k(v)-W_k(u)$ and $C_k = W_k(u) \cap W_k(v)$. Clearly the Jaccard coefficient at $k$-hops
%%can be expressed as follows:
%%\begin{equation}
%%	J_k(u,v) = \frac{|C_k|}{|A_k| + |B_k| + |C_k|}
%%\end{equation}
%
%To deduce the relationship between $J_k(u,v)$ and $J_{k+1}(u,v)$, we prove the following lemma first:
%\begin{theorem}
%Let $S$ be a collection of connected vertexes, the number of newly discovered
%vertexes by one-hop expansion from $S$ is bounded by a function on $|S|$.
%\end{theorem}
%\begin{proof}
%Consider a random variable $Y_i$ indicate the newly 
%discovered vertexes from one-hop expansion from vertex $i$. 
%Then the probability of $|Y_i|=y$ is can be analyzed as follows: there 
%are $d_i$ edges for vertex $i$. Since $|Y_i|$ is connected with $S$, one edge
%is fixed to link with a vertex in $S$. There are remaining $d_i-1$ edges with
%$y$ edges linked to the new vertexes. In total, there are $|V|-1 \choose d_i -1$
%combinations with $d_i$ edges. Therefore, the probability can be written as:
%\begin{equation}
%Prob(Y_i = y| v_i \in S) = \frac{{|S| -1 \choose d_i - y -1}{|V|-|S| \choose y}}{{|V|-1 \choose d_i -1}}
%\end{equation}
%Thus, the expectation of $Y_i$ is:
%\begin{equation}
%\begin{split}
%E(Y_i|v_i \in S) & = \Sigma( y Prob(Y_i = y| v_i \in S) )\\
%	& = \Sigma_{y=1}^{y=d_i -1} ( \frac{{|S| -1 \choose d_i - y -1}{|V|-|S| \choose y}}{{|V|-1 \choose d_i -1}} y )\\
%	& = \Sigma_{y=1}^{y=d_i -1} (\frac{{|S| -1 \choose d_i - y -1}{|V|-|S| -1 \choose y - 1}}{{|V|-1 \choose d_i -1}}  (|V|-|S|))\\
%	& = (|V|-|S|)  \Sigma_{y=1}^{y=d_i -1}\frac{{|S| -1 \choose d_i - y -1}{|V|-|S| -1 \choose y - 1}}{{|V|-1 \choose d_i -1}} \\
%	& = (|V|-|S|)  \frac{{|V|-2 \choose d_i - 2}}{{|V|-1 \choose d_i -1}} = \frac{(|V|-|S|)(d_i-1)}{|V| - 1}
%\end{split}
%\end{equation}
%Taking the expectation over all vertexes in $S$, we can find the expectation of $E(Y|S) = \frac{(|V|-|S|)*(\overline{d}-1)}{|V| - 1}$, 
%where $\overline{d}$ is the average degree of the graph. We then define the event $X=\cup_{i=1}^{i=|S|} Y_i$, i.e. $X$ is the number
%of newly discovered vertexes for one-hop expansion of entire $S$. By union bound, 
%the expectation of $X$ is:
%\begin{equation} 
%\begin{split}
%E(X|S) &= E(\cup_{i=1}^{i=|S|}Y_i|S) \\
%& \leq \Sigma_{i=1}^{i=|S|}E(Y_i|S) \\ 
%&= \frac{|S|(|V|-|S|)(\overline{d} -1)}{|V|-1} = f(|S|)
%\end{split}
%\end{equation}
%The bound is achieved when each vertex in $S$ discovers non-overlapping neighbors, such
%as in the case of tree structure. Therefore, the newly discovered vertexes are tightly bounded 
%by a quadratic function on $|S|$.
%\end{proof}
%
%We thus use $f(m)$ to denote the number of newly discovered vertexes
%from a base set of $m$ connected vertexes. Since $u,v$ have 
%identical degree distribution, their expected value $S_u=E(|W_k(u)|)$
%and $S_v=E(|W_k(v)|)$ are the same, i.e. $S_u = S_v$.
%We further use $\alpha = \frac{|C|}{|A|+|C|}$ to denote the portion of shared
%components in $u$'s $k$-hop neighborhood. Likewise, we use $\beta = \frac{|B|}{|B|+|C|}$ for 
%$W_k(v)$. Since vertexes have identical degree distribution, $\alpha$ and $\beta$ are likely
%to be the same, i.e. $\alpha \cong \beta$.
%Now, the $J_{k+1}(u,v)$ for ($k+1$)-hop can be represented as follows:
%\begin{equation}
%\begin{split}
%J_{k+1}(u,v) & = \frac{|C_{k+1}|}{|A_{k+1}| + |B_{k+1}| + |C_{k+1}|} \\
%	& = \frac{|C_k| + \alpha * f(S_u) + \beta * f(S_v)}{|A_k| +|B_k| +|C_k| +f(S_u) +  f(S_v) -\Delta} 
%\end{split}
%\end{equation}
%, the $\Delta$ here is to compensate the doubly counted portion on the
%overlapping: $(A_{k+1} \cup B_{k+1}) \cap C_{k+1}$. Since $\Delta \geq 0$, by dropping the $\Delta$, it  follows:
%\begin{equation}
%\begin{split}
%J_{k+1}(u,v) & \geq \frac{|C_k| + \alpha * f(S_u) + \beta * f(S_v)}{|A_k| +|B_k| +|C_k| +f(S_u) +  f(S_v)}  \\
%		& = \frac{|C_k| + 2\alpha * f(S_u)}{|A_k|+|B_k|+|C_k| + 2 * f(S_u)}
%\end{split}
%\end{equation}
%Due to the fact that $\alpha = \frac{|C_k|}{|C_k|+|A_k|} \geq \frac{|C_k|}{|A_k|+|B_k|+|C_k|}$, it follows:
%\begin{equation}
%\begin{split}
%	\frac{\alpha f(S_u)}{f(S_u)} & \geq \frac{|C_k|}{|A_k|+|B_k|+|C_k|} \Rightarrow \\
%	\frac{|C_k| + 2\alpha * f(S_u)}{|A_k|+|B_k|+|C_k| + 2 * f(S_u)} & \geq \frac{|C_k|}{|A_k|+|B_k|+|C_k|} \\
%	& = J_k(u,v)
%\end{split}
%\end{equation}
%
%Therefore, our analysis shows that $J_k(u,v)$ is most likely increasing for random graphs with identical
%degree distribution.
%
%%If each $d_i$ follows the same distribution, we have a simplified form of $E(Y) =
%%|S|*E(d) * \frac{|V|-|S|}{|V|}$ or simply $E(Y)= O(1 - \frac{|S|}{|V|})*|S|$.  Using this result, we can deduce the $J_{k+1}$ as
%%follows:
%%
%%\begin{equation}
%%	J_{k+1}(u,v) = \frac{\beta_C |C_k|}{\beta_A |A_K| + \beta_B |B_K| + \beta_C|C_K|}
%%\end{equation} 
%%,where $\beta_C$ is proportional to $|C_K|$ and $\beta_A$ ($\beta_B$) is proportional to $|A|$ ($|B|$).
%%
%%
%%We assume the average degree for each vertex is $\overline{d}$, so that when a vertex
%%performs one-hop expansion, the newly discovered vertices are $|\overline{d}|$.
%%We then assert that the Jaccard coefficient between two vertex $u$ and $v$ increase with
%%number of hops becomes bigger. We use $J_k(u,v)$ to be the Jaccard coefficient for the
%%$k^{th}$ neighborhood of $u$ and $v$. Then we assert the following:
%%\begin{equation}
%%	J_k(u,v) < J_{k+1}(u,v)
%%\end{equation}
%%
%%For any vertex pair $u$ and $v$, their intersection on $k$-hop window consists of three 
%%set of vertices, namely $A_k = W_k(u)-W_k(v)$, $B_k=W_k(v)-W_k(u)$ and $C_k= W_k(v) \cap W_k(u)$.
%%By definition, we have the following:
%%\begin{equation}
%%\begin{split}
%%J_k(u,v) &  = \frac{|C_k|}{|A_k|+|B_k|+|C_k|} \\
%%J_{k+1}(u,v) &  = \frac{|C_{k+1}|}{|W_{k+1}(u) \cup W_{k+1}(v)|}
%%\end{split}
%%\end{equation}
%%To build connections between $J_k(u,v)$ and $J_{k+1}(u,v)$, we define 
%%\begin{equation}
%%\begin{split}
%%G & =\{v| v_1\in A_k, 
%%\exists v_2 \in C_k, dist(v_1,v_2) = 1\}
%%\\
%%H & =\{v| v_1\in B_k, 
%%\exists v_2 \in C_k, dist(v_1,v_2) = 1\}
%%\end{split}
%%\end{equation}
%%$G$ (resp. $H$) represents the portion of vertices in $A_k$ (resp. $B_k$) which
%%are of 1-hop distance away from any vertices in $C_k$. 
%%$W_{k+1}(u) \cup W_{k+1}(v)$ can be then derived from the portion of $A_k$
%%,$B_k$ and $C_k$ by 1-hop expansion, it follows:
%%\begin{equation}
%%\begin{split}
%%|W_{k+1}(u) \cup W_{k+1}(v)| & = \overline{d} * (|A_k| +|B_k| + |C_k|)  \\
%%			& - |G| - |H|
%%\end{split}
%%\end{equation}
%%The deduction on $|G|$ and $|H|$ is to compensate on the double counted vertices.
%%Therefore, it follows:
%%\begin{equation}
%%\begin{split}
%%J_{k+1}(u,v)  & = \frac{|C_{k+1}|}{|W_{k+1}(u) \cup W_{k+1}(v)|} \\
%%			& = \frac{|C_{k}| * \overline{d}}{(|A_k| +|B_k| + |C_k|)*\overline{d} - |G| - |H|} \\
%%			& \geq \frac{|C_{k}| * \overline{d}}{(|A_k| +|B_k| + |C_k|)*\overline{d}} \\
%%			& = \frac{|C_k|}{|A_k| + |B_k| + |C_k|} = J_k(u,v)
%%\end{split}
%%\end{equation}
%%The above derivation is based on the undirected graph; However, in many real directed graphs
%%e.g. Facebook friendship graph, it contains many reciprocal edges, in such scenarios, the
%%above derivation mostly holds. We verified our scheme in two real-life graphs in Section~\ref{sec:experiments}.
%%
%%Assume every vertex $v_i$'s degree is a random variable $s_i$ and 
%%Let $\overline{s}$ be its expectation of $s_i$. Then, the cardinality
%%of 1-hop expansion of a set of vertices $V$ can be written as 
%%$EX(V) = |\cap_{v_i \in V} s_i|$. It follows that $E(EX(V)) \leq \overline{s}*|V|$.
%\eat{In the following, we justify the soundness of the approximation technique in $EMC$.
%Consider the Jaccard coefficient for two $k$-hop windows wrt nodes $u$ and $v$, denoted by $J_k(u,v)$.
%For convenience, let $A$, $B$, and $C$, denote 
%the sets $W(u)$-$W(v)$, 
%$W(v)$-$W(u)$, and 
%$W(u) \cap W(v)$, respectively. 
%Thus, $J_k(u,v)$ can be rewritten as $\frac{|C|}{|A|+|B|+|C|}$, where $|\centerdot|$ denotes the cardinality of a set. 
%Since a \emph{(k+1)-hop} window can be viewed as a \emph{1-hop} 
%expansion from a \emph{k-hop} window, the $J_{k+1}(u,v)$ can be estimated by:
%\begin{equation} \label{eq:jacc-esimation}
%\begin{split}
%J_{k+1}(u,v) & = \frac{\alpha |C| + \Delta}{\beta |A|+ \beta |B|+\alpha |C| - \Delta} \\
%\end{split}
%\end{equation}  
%Here, $\alpha$ denotes the expansion factor of $C$,
%and $\beta$ denotes the expansion factor of $A$ and $B$. 
%The expansion factor measures the additional number of nodes that are added to a set based on the
%\emph{1-hop} expansion of that set. 
%$\Delta$ denotes the additional nodes that are common in the the expanded sets $A$ and $B$ from  their \emph{1-hop} expansions. 
%On average, both $\alpha$ and $\beta$ should be close to the average degree of the graph. 
%Thus, this shows that $J_{k+1}(u,v) > J_{k}(u,v)$. 
%In other words, if two nodes are grouped into the same cluster wrt to their k-hop windows,
%then these two nodes are likely to be also grouped into the same cluster as the value of $k$ increases. 
%}



%\subsection{Handling Updates}
%In this section, we overview how our DBIndex is maintained when there are updates to the input graph.
%There are two types of updates for graph data: updates to the attribute values associated with the nodes/edges and updates to the graph structure 
%(e.g., addition/removal of nodes/edges).
%Since the DBIndex is an index on the graph structure which is independent of the attribute values in the graph,
%the DBIndex is not affected by updates to the graph's attribute values.
%
%The efficient maintenance of the DBIndex in the presence of structural updates is challenging as a single structural change (e.g., adding an edge) could affect many vertex windows.  To balance the trade-off between efficiency of index update and efficiency of query processing, 
%we have adopted a two-phase approach to maintain the DBIndex.
%The first phase is designed to optimize update efficiency where the DBIndex is updated incrementally whenever there are structural updates to the graph.
%The incremental index update ensures that the updated index functions correctly but does not fully optimize the query efficiency of the updated index
%in terms of maximizing the shared computations.
%The second phase is designed to optimize query efficiency where the DBIndex is periodically re-organized to maximize share computations.
%
%As an example of how the DBIndex is updated incrementally, consider a structural change where a new edge is added to the input graph.
%Let $S$ denote the subset of graph vertexes whose windows have expanded (with additional vertexes) as a result of the insertion of the new edge.
%Let $W'(v)$ denote the set of additional vertexes in the vertex window of $v$ for each vertex $v \in S$.
%Based on the identified changes to the vertex windows (i.e., $S$ and $\{W'(v) |\ v \in S\}$), 
%we construct a secondary DBIndex which is then merged into the primary DBIndex.
%As the identified changes are small relative to the entire graph and collection of vertex windows,
%the construction and merging of the secondary index can be processed efficiently relative to an index reorganization to fully optimize query efficiency.
